{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251db5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # 1. DEFINE THE WORLD\n",
    "# grid_size = 5\n",
    "# start_pos = (0, 0)  # Top-left corner (row, column)\n",
    "# goal_pos = (4, 4)   # Bottom-right corner\n",
    "\n",
    "# # 2. DEFINE ACTIONS: Up, Down, Left, Right\n",
    "# actions = {\n",
    "#     0: (-1, 0),  # Up: subtract 1 from row\n",
    "#     1: (1, 0),   # Down: add 1 to row\n",
    "#     2: (0, -1),  # Left: subtract 1 from column\n",
    "#     3: (0, 1)    # Right: add 1 to column\n",
    "# }\n",
    "# num_actions = len(actions)\n",
    "\n",
    "# # 3. THE AGENT'S BRAIN: Q-Table\n",
    "# # The table has one row for every possible cell (5x5 = 25 states)\n",
    "# # and one column for each action (4 actions).\n",
    "# q_table = np.zeros((grid_size * grid_size, num_actions))\n",
    "# print(\"Agent's starting Q-Table (empty brain):\")\n",
    "# print(q_table)\n",
    "\n",
    "# # 4. A FUNCTION TO SIMULATE A STEP\n",
    "# def take_action(state, action_id):\n",
    "#     \"\"\"Simulates moving the agent in the grid.\n",
    "#     Returns: new_state, reward, done\"\"\"\n",
    "#     # Convert state number (0-24) back to (row, col) coordinates\n",
    "#     row = state // grid_size  # Integer division\n",
    "#     col = state % grid_size   # Remainder\n",
    "\n",
    "#     # Get the move (e.g., (0, 1) for Right)\n",
    "#     move = actions[action_id]\n",
    "#     new_row = row + move[0]\n",
    "#     new_col = col + move[1]\n",
    "\n",
    "#     # Check if the new position is valid (inside the grid)\n",
    "#     if 0 <= new_row < grid_size and 0 <= new_col < grid_size:\n",
    "#         new_state = new_row * grid_size + new_col # Convert back to a state number\n",
    "#     else:\n",
    "#         new_state = state  # If move is invalid, stay in place\n",
    "\n",
    "#     # Check if the new state is the GOAL\n",
    "#     if new_state == (goal_pos[0] * grid_size + goal_pos[1]):\n",
    "#         reward = 10\n",
    "#         done = True\n",
    "#         print(f\"ðŸŽ‰ Goal reached! Agent gets a big reward: {reward}\")\n",
    "#     else:\n",
    "#         reward = -1  # Small punishment for every step to encourage faster paths\n",
    "#         done = False\n",
    "\n",
    "#     return new_state, reward, done\n",
    "\n",
    "# # 5. LET'S MANUALLY TEST ONE STEP\n",
    "# print(\"\\n--- Let's test one step ---\")\n",
    "# current_state = 0 # State 0 is (0,0) - the start\n",
    "# print(f\"Agent is at state: {current_state} (which is position {start_pos})\")\n",
    "# chosen_action = 3 # Let's choose action 3: \"Right\"\n",
    "# print(f\"Agent chooses action: {chosen_action} (Right)\")\n",
    "\n",
    "# new_state, reward, done = take_action(current_state, chosen_action)\n",
    "# print(f\"Result: New State: {new_state}, Reward: {reward}, Episode Done? {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77f1eeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<00:00, 46793.57episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training Finished!\n",
      "Average steps per episode (last 1000): 4.57\n",
      "Average reward per episode (last 1000): 96.43\n",
      "\n",
      "ðŸ§  Agent's Learned Policy (best action for each state):\n",
      "â†“ â† â† â† â†“ \n",
      "â†“ â†‘ â† â† â† \n",
      "â†“ â†“ â† â† â†‘ \n",
      "â†“ â† â† â†‘ â†’ \n",
      "â†‘ â† â†‘ â† â† \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm # For the progress bar\n",
    "\n",
    "# 1. DEFINE THE WORLD\n",
    "grid_size = 5\n",
    "start_state = 0  # Top-left corner (0, 0)\n",
    "goal_state = 20  # Bottom-right corner (4, 4)\n",
    "\n",
    "# 2. DEFINE ACTIONS: Up, Down, Left, Right\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up: subtract 1 from row\n",
    "    1: (1, 0),   # Down: add 1 to row\n",
    "    2: (0, -1),  # Left: subtract 1 from column\n",
    "    3: (0, 1)    # Right: add 1 to column\n",
    "}\n",
    "num_actions = len(actions)\n",
    "\n",
    "# 3. THE AGENT'S BRAIN: Q-Table\n",
    "# The table has one row for every possible cell (5x5 = 25 states)\n",
    "# and one column for each action (4 actions).\n",
    "q_table = np.zeros((grid_size * grid_size, num_actions))\n",
    "\n",
    "# 4. A FUNCTION TO SIMULATE A STEP\n",
    "def take_action(state, action_id):\n",
    "    \"\"\"Simulates moving the agent in the grid.\n",
    "    Returns: new_state, reward, done\"\"\"\n",
    "    # Convert state number (0-24) back to (row, col) coordinates\n",
    "    row = state // grid_size\n",
    "    col = state % grid_size\n",
    "\n",
    "    # Get the move (e.g., (0, 1) for Right)\n",
    "    move = actions[action_id]\n",
    "    new_row = row + move[0]\n",
    "    new_col = col + move[1]\n",
    "\n",
    "    # Check if the new position is valid (inside the grid)\n",
    "    if 0 <= new_row < grid_size and 0 <= new_col < grid_size:\n",
    "        new_state = new_row * grid_size + new_col\n",
    "    else:\n",
    "        new_state = state  # If move is invalid, stay in place\n",
    "\n",
    "    # Check if the new state is the GOAL\n",
    "    if new_state == goal_state:\n",
    "        reward = 100  # Big reward for reaching the goal!\n",
    "        done = True\n",
    "    else:\n",
    "        reward = -1   # Small punishment for every step to encourage faster paths\n",
    "        done = False\n",
    "\n",
    "    return new_state, reward, done\n",
    "\n",
    "# 5. HYPERPARAMETERS FOR THE LEARNING ALGORITHM\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99  # How much we care about future rewards\n",
    "exploration_rate = 0.1  # Epsilon: 10% chance to explore\n",
    "num_episodes = 20000    # How many full games to play\n",
    "\n",
    "# 6. MAIN TRAINING LOOP\n",
    "print(\"ðŸŸ¢ Starting Training...\")\n",
    "# Lists to track performance\n",
    "rewards_per_episode = []\n",
    "steps_per_episode = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training\", unit=\"episode\"):\n",
    "    # Reset the game for a new episode\n",
    "    current_state = start_state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    # Play one full game (episode) until done\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        # Choose action: Explore or Exploit?\n",
    "        if np.random.random() < exploration_rate:\n",
    "            action = np.random.randint(num_actions)  # Explore: random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[current_state]) # Exploit: best action\n",
    "\n",
    "        # Take the action, get new state and reward\n",
    "        new_state, reward, done = take_action(current_state, action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update Q-table using the Q-learning formula\n",
    "        old_value = q_table[current_state, action]\n",
    "        next_max = np.max(q_table[new_state])\n",
    "        \n",
    "        # Q-learning formula: New Q = Old Q + LR * (Reward + DF * Best Future Q - Old Q)\n",
    "        new_value = old_value + learning_rate * (reward + discount_factor * next_max - old_value)\n",
    "        q_table[current_state, action] = new_value\n",
    "\n",
    "        # Move to the new state\n",
    "        current_state = new_state\n",
    "\n",
    "    # Record the results of this episode\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    steps_per_episode.append(steps)\n",
    "\n",
    "# 7. SHOW FINAL RESULTS\n",
    "print(\"\\nâœ… Training Finished!\")\n",
    "print(f\"Average steps per episode (last 1000): {np.mean(steps_per_episode[-1000:]):.2f}\")\n",
    "print(f\"Average reward per episode (last 1000): {np.mean(rewards_per_episode[-1000:]):.2f}\")\n",
    "\n",
    "# 8. SHOW THE LEARNED POLICY (what the agent will do in each state)\n",
    "print(\"\\nðŸ§  Agent's Learned Policy (best action for each state):\")\n",
    "for row in range(grid_size):\n",
    "    for col in range(grid_size):\n",
    "        state = row * grid_size + col\n",
    "        best_action = np.argmax(q_table[state])\n",
    "        action_symbol = ['â†‘', 'â†“', 'â†', 'â†’'][best_action]\n",
    "        print(f\"{action_symbol} \", end='')\n",
    "    print()  # New line after each row\n",
    "\n",
    "# 9. (Optional) You can add plotting here to see the improvement over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbf90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Now watching trained agent play...\n",
      "\u001b[H\u001b[JStep 1:\n",
      "\n",
      "-------------\n",
      "|ðŸ¤–â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—»ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—»ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—»ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|ðŸŽ¯â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "-------------\n",
      "Action: â†“\n",
      "\u001b[H\u001b[JStep 2:\n",
      "\n",
      "-------------\n",
      "|ðŸš¦â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|ðŸ¤–â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—»ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—»ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|ðŸŽ¯â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "-------------\n",
      "Action: â†“\n",
      "\u001b[H\u001b[JStep 3:\n",
      "\n",
      "-------------\n",
      "|ðŸš¦â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—¼ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|ðŸ¤–â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—»ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|ðŸŽ¯â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "-------------\n",
      "Action: â†“\n",
      "\u001b[H\u001b[JStep 4:\n",
      "\n",
      "-------------\n",
      "|ðŸš¦â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—¼ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—¼ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|ðŸ¤–â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|ðŸŽ¯â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "-------------\n",
      "Action: â†“\n",
      "\u001b[H\u001b[Jâœ… Episode Complete!\n",
      "\n",
      "-------------\n",
      "|ðŸš¦â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—¼ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—¼ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|â—¼ï¸â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "|ðŸ¤–â—»ï¸â—»ï¸â—»ï¸â—»ï¸|\n",
      "-------------\n",
      "Total steps: 5\n",
      "Final reward: 100\n"
     ]
    }
   ],
   "source": [
    "# 10. VISUALIZATION: WATCH THE TRAINED AGENT PLAY\n",
    "def draw_grid(state, path=[]):\n",
    "    \"\"\"Draws the grid world, showing the agent's current position and path.\"\"\"\n",
    "    print(\"\\n\" + \"-\" * (grid_size * 2 + 3))\n",
    "    for row in range(grid_size):\n",
    "        print(\"|\", end=\"\")\n",
    "        for col in range(grid_size):\n",
    "            current_pos = row * grid_size + col\n",
    "            if current_pos == state:\n",
    "                print(\"ðŸ¤–\", end=\"\")  # Agent\n",
    "            elif current_pos == start_state:\n",
    "                print(\"ðŸš¦\", end=\"\")  # Start\n",
    "            elif current_pos == goal_state:\n",
    "                print(\"ðŸŽ¯\", end=\"\")  # Goal\n",
    "            elif current_pos in path:\n",
    "                print(\"â—¼ï¸\", end=\"\")  # Visited path\n",
    "            else:\n",
    "                print(\"â—»ï¸\", end=\"\")  # Empty cell\n",
    "        print(\"|\")\n",
    "    print(\"-\" * (grid_size * 2 + 3))\n",
    "\n",
    "def run_episode_with_visualization():\n",
    "    \"\"\"Runs one episode showing each step visually.\"\"\"\n",
    "    print(\"\\nðŸŽ¬ Now watching trained agent play...\")\n",
    "    current_state = start_state\n",
    "    done = False\n",
    "    path = [current_state]\n",
    "    \n",
    "    while not done:\n",
    "        # Clear screen for better animation (works in most terminals)\n",
    "        print(\"\\033[H\\033[J\", end=\"\")\n",
    "        \n",
    "        print(f\"Step {len(path)}:\")\n",
    "        draw_grid(current_state, path)\n",
    "        \n",
    "        # Choose best action from Q-table\n",
    "        action = np.argmax(q_table[current_state])\n",
    "        action_symbol = ['â†‘', 'â†“', 'â†', 'â†’'][action]\n",
    "        print(f\"Action: {action_symbol}\")\n",
    "        \n",
    "        # Take the action\n",
    "        new_state, reward, done = take_action(current_state, action)\n",
    "        \n",
    "        # Add to path and update state\n",
    "        if new_state not in path:  # Avoid duplicates in path\n",
    "            path.append(new_state)\n",
    "        current_state = new_state\n",
    "        \n",
    "        # Wait a bit so human can see\n",
    "        input(\"Press Enter for next step...\")\n",
    "    \n",
    "    # Show final result\n",
    "    print(\"\\033[H\\033[J\", end=\"\")\n",
    "    print(\"âœ… Episode Complete!\")\n",
    "    draw_grid(current_state, path)\n",
    "    print(f\"T tal steps: {len(path)}\")\n",
    "    print(f\"Final reward: {reward}\")\n",
    "\n",
    "# Run the visualization\n",
    "run_episode_with_visualization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
